<h1 id=>SCARA Robot Mounted on Order Picker</h1>
<p>
    This is a simulation on Unity for testing computer vision and control using ROS. The connection between Unity and ROS  has been accomplished 
    through the <a href="https://github.com/Unity-Technologies/ROS-TCP-Connector"> ROS-TCP-Connector</a> (in Unity) and the
    <a href="https://github.com/Unity-Technologies/ROS-TCP-Endpoint"> ROS-TCP-Endpoint</a> (in ROS). 
    The robot implmets a neural network that estimates the pallet's position allowing the robot to pick boxes from the pallet.
</p>

<iframe width="600" height="350" src="https://user-images.githubusercontent.com/47704357/170530414-e110a1dc-acf2-4b17-8f04-46f130e872e7.mp4" frameborder="0" allowfullscreen></iframe>

<!-- SHOW UI AND ORDER PICKER WORKING -->
<br>
<iframe width="600" height="350" src="https://user-images.githubusercontent.com/47704357/170529145-65fb159d-e98d-4a26-a011-6847f195734c.mp4" frameborder="0" allowfullscreen></iframe>
<iframe width="600" height="350" src="https://user-images.githubusercontent.com/47704357/170529161-f1be2d1e-a9ba-4278-a159-093f2b93602d.mp4" frameborder="0" allowfullscreen></iframe>
<br>
<iframe width="600" height="350" src="https://user-images.githubusercontent.com/47704357/170529168-92c62eed-38e0-48e4-a91e-f7dbda7e281e.mp4" frameborder="0" allowfullscreen></iframe>
<iframe width="600" height="350" src="https://user-images.githubusercontent.com/47704357/170529178-72fc167c-9dd7-48d2-a71e-a65ee2dd233d.mp4" frameborder="0" allowfullscreen></iframe>
<br>

<h2>CAD</h2>
<p>The robot was designed in SolidWorks </p>
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/closeUpCAD.png" alt="CAD">
<br>
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/closeUpCAD2.png" alt="CAD">


<h2>Data generation & Neural Network</h2>
<!-- INSERT 3 images in a row -->
<p>Generated datasets example:</p>
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/DS1.png" alt="">
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/DS2.png" alt="">
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/DS3.png" alt="">

<p>
    The data for training the network was generated using 
    <a href="https://github.com/Unity-Technologies/com.unity.perception">Unity's Perception Package</a>, 
    and the neural network was implmented and trained using Tensorflow. The architecture of the network is based on
    that described on the <a href="https://arxiv.org/pdf/1703.06907.pdf">"Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"</a> paper
    and the architecture described on <a href="https://github.com/Unity-Technologies/Robotics-Object-Pose-Estimation/tree/main/Model">Unity's tutorial for pose estimation</a>
    based on the same paper.
</p>

<p>
    Two versions of the network were implemented. One was trained on a "simpler" version of the problem, where there was only one pallet.
    The other was trained on a "more complete" version of the problem, where there were multiple pallets and shelves.
</p>

<!-- INSERT GIF OF DATA GENERATION -->
<h3>For the first version of the network:</h3>
<h4>Random pose data generation</h4>
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/DatagenRandomPose_1.gif" alt="Random pose data generation">
<h4>Random pose and light data generation</h4>
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/DatagenRandomLight_1.gif" alt="Random pose and light data generation">
<h4>Domain randomization data generation</h4>
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/DataGenDomainRandom_1.gif" alt="Domain randomization data generation">

<h3>For the second version of the network:</h3>
<h4>Random pallets data generation</h4>
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/DatagenShelvingEnabled_1.gif" alt="Random pose data generation">
<h4>Random pallets and light data generation</h4>
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/DatagenShelvingLight_1.gif" alt="Random pose and light data generation">
<h4>Random pallets, light and position data generation</h4>
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/DatagenShelvingPos_1.gif" alt="Domain randomization data generation">

<h2>UI</h2>
<p>
    UI runs on Windows, Linux, Android, IOS and web. Although the video stream does not work on the web, the UI is still usable.
</p>

<!-- <img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/screenShotUI.png" alt="UI screenshot 1">
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/screenShotUI2.png" alt="UI screenshot 2">
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/screenShotUI3.png" alt="UI screenshot 3"> -->

<p>
    The UI was developed using Flutter. The UI is a web application that allows the user to control the robot and pick boxes 
    from the pallet when the robot is in position. The UI connects with the robot through Flask wich instantiates a ROS node that
    controls the robot, plans trajectories, manages the messages sent from the robot's sensores and sends the messages back to the UI.
</p>
<img src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/screenShotUI4.png" alt="UI screenshot 4">
<!-- INSERT UI vetical, horizontal video and image of interactions -->
<br>
<!-- <img width="100%" src="https://github.com/Exusai/bodeg-on/raw/master/assetsForWebsite/diagrama.png" alt="From UI to Simulation"> -->